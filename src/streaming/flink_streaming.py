import os
import sys
import logging
from pathlib import Path
from typing import List

from loguru import logger
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.watermark_strategy import WatermarkStrategy
from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode
from pyflink.datastream.connectors.kafka import (
    KafkaOffsetsInitializer,
    KafkaSource,
)

from postgres import PostgreSQLClient

# Configure logging for Docker/Airflow compatibility
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stdout,
    force=True
)

JARS_PATH = Path(os.getcwd()) / "jars"  # S·ª≠ d·ª•ng Path ƒë·ªÉ d·ªÖ x·ª≠ l√Ω

def get_jar_uri(jar_name: str) -> str:
    """T·∫°o file URI ƒë√∫ng cho JAR tr√™n Windows/Linux."""
    jar_path = JARS_PATH / jar_name
    if not jar_path.exists():
        raise FileNotFoundError(f"JAR kh√¥ng t·ªìn t·∫°i: {jar_path}")
    posix_path = str(jar_path).replace("\\", "/")  # Chuy·ªÉn sang forward slash
    if os.name == "nt":  # Windows
        return f"file:///{posix_path}"
    else:
        return f"file://{posix_path}"

def main():
    logging.info("üöÄ Flink streaming job started")
    sys.stdout.flush()  # Force flush ƒë·ªÉ Airflow capture log ngay
    
    try:
        # Test ƒë·ªÉ container ch·∫°y l√¢u h∆°n v√† c√≥ th·ªÉ debug
        import time
        logging.info("Initializing Flink environment...")
        time.sleep(2)
        
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_runtime_mode(RuntimeExecutionMode.STREAMING)

        # Add required JARs v·ªõi URI ƒë√∫ng (lo·∫°i b·ªè SQL connector ƒë·ªÉ tr√°nh conflict)
        logging.info("Loading JARs...")
        env.add_jars(
            get_jar_uri("flink-connector-kafka-3.2.0-1.18.jar"),  # N√¢ng c·∫•p connector
            get_jar_uri("kafka-clients-3.2.0.jar"),  # Client version >=3.1.0 ƒë·ªÉ fix method
            get_jar_uri("flink-json-1.18.0.jar"),
            get_jar_uri("flink-connector-jdbc-3.2.0-1.18.jar"),  # N√¢ng c·∫•p n·∫øu c·∫ßn
            get_jar_uri("postgresql-42.7.7.jar"),
        )

        logging.info("Configuring Kafka source...")
        source = (
            KafkaSource.builder()
            .set_bootstrap_servers("broker:29092")  # Updated to use service name
            .set_topics("binance.c2c.trades")
            .set_group_id("flink-datastream-consumer-001")
            .set_starting_offsets(KafkaOffsetsInitializer.earliest())
            .set_value_only_deserializer(SimpleStringSchema())
            .build()
        )

        logging.info("Creating data stream...")
        ds = env.from_source(source, WatermarkStrategy.no_watermarks(), "kafka-source")
        ds.print()

        logging.info("Executing Flink job...")
        env.execute("kafka-source-print")
        
        logging.info("‚úÖ Flink streaming job completed successfully")
        sys.stdout.flush()
        
    except Exception as e:
        logging.error(f"‚ùå Flink streaming job failed: {e}", exc_info=True)
        sys.stdout.flush()
        raise


if __name__ == "__main__":
    main()