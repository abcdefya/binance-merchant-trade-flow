# =====================================================================
# SILVER JOB - PRODUCTION VERSION (Patched, Optimized, Stable, Fast)
# Author: ChatGPT (Senior Data Engineer)
# =====================================================================

import os
import logging
from decimal import Decimal
from typing import Optional

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, lit, lower, upper, when, coalesce,
    from_unixtime, to_date, year, month, dayofmonth,
    array, map_from_arrays, current_date
)
from pyspark.sql.types import DecimalType

from delta.tables import DeltaTable
from spark_utils import create_spark_session, check_minio_connectivity


# ========================= CONFIG & LOGGING ============================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info("Starting Silver Job (Production Version)")

MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
ACCESS_KEY = os.getenv("MINIO_ROOT_USER", os.getenv("AWS_ACCESS_KEY_ID", ""))
SECRET_KEY = os.getenv("MINIO_ROOT_PASSWORD", os.getenv("AWS_SECRET_ACCESS_KEY", ""))
BRONZE_PATH = os.getenv("BRONZE_PATH", "s3a://bronze/c2c_trades/")
SILVER_PATH = os.getenv("SILVER_PATH", "s3a://silver/c2c_trades/")
INPUT_PATH = os.getenv("INPUT_PATH", "/shared_volume/c2c/latest/*.parquet")

logger.info(f"BRONZE_PATH={BRONZE_PATH}")
logger.info(f"SILVER_PATH={SILVER_PATH}")
logger.info(f"INPUT_PATH={INPUT_PATH}")


# ========================= STATUS PRIORITY MAP =========================

PRIORITY_MAP = {
    "COMPLETED": 8,
    "IN_APPEAL": 7,
    "DISTRIBUTING": 6,
    "BUYER_PAYED": 5,
    "TRADING": 4,
    "PENDING": 3,
    "CANCELLED": 2,
    "CANCELLED_BY_SYSTEM": 1,
    "UNKNOWN": 0
}


def build_priority_expr():
    """Return Spark expression map(order_status -> priority)."""

    return map_from_arrays(
        array([lit(k) for k in PRIORITY_MAP.keys()]),
        array([lit(v).cast("int") for v in PRIORITY_MAP.values()])
    )


# ========================= TRANSFORMATION ===============================

def extract_timestamp(df: DataFrame) -> DataFrame:
    """Standardize timestamp column -> trade_date, trade_time."""
    ts_col = None
    for c in ("createTime", "create_time", "create_time_ms"):
        if c in df.columns:
            ts_col = c
            break

    if ts_col is None:
        logger.warning("No timestamp column found → using current_date")
        return (df
                .withColumn("trade_time", lit(None))
                .withColumn("trade_date", current_date()))

    # Convert ms → seconds + UTC+7
    df = df.withColumn("trade_time",
                       from_unixtime((col(ts_col) / 1000) + 7 * 3600))
    df = df.withColumn("trade_date", to_date(col("trade_time")))
    return df.drop(ts_col)


def cast_numeric(df: DataFrame, col_name: str, precision=18, scale=8) -> DataFrame:
    """Safe cast, fallback to 0."""
    if col_name in df.columns:
        return df.withColumn(col_name, col(col_name).cast(DecimalType(precision, scale)))
    return df.withColumn(col_name, lit(Decimal("0")))


def transform_silver(df: DataFrame) -> DataFrame:
    """Main transformation logic for Silver layer."""

    df = extract_timestamp(df)

    # ----- Standardize price columns -----
    price_map = {
        "totalPrice": "total_price",
        "total_price": "total_price",
        "unitPrice": "unit_price",
        "unit_price": "unit_price",
    }
    for src, dst in price_map.items():
        if src in df.columns:
            df = df.withColumn(dst, col(src))

    df = cast_numeric(df, "total_price", 18, 2)
    df = cast_numeric(df, "unit_price", 18, 2)
    df = cast_numeric(df, "amount", 18, 8)
    df = cast_numeric(df, "commission", 18, 8)

    df = df.withColumn("asset", lower(col("asset")))
    df = df.withColumn("fiat", lower(col("fiat")))

    # ----- Compute net_volume -----
    if "tradeType" in df.columns:
        df = df.withColumn("trade_type", upper(col("tradeType")))
        df = df.drop("tradeType")
    elif "trade_type" in df.columns:
        df = df.withColumn("trade_type", upper(col("trade_type")))

    df = df.withColumn(
        "net_volume",
        when(col("trade_type") == "BUY", col("amount") - col("commission"))
        .when(col("trade_type") == "SELL", -(col("amount") + col("commission")))
        .otherwise(lit(Decimal("0")))
    )

    df = df.withColumn("total_vnd", col("total_price"))

    # ----- Add partition columns -----
    df = df.withColumn("year", year(col("trade_date")))
    df = df.withColumn("month", month(col("trade_date")))
    df = df.withColumn("day", dayofmonth(col("trade_date")))

    # ----- Add status priority column -----
    priority_expr = build_priority_expr()
    df = df.withColumn(
        "status_priority",
        priority_expr[col("order_status")]
    )

    return df


# ========================= MERGE LOGIC ==================================

def merge_into_silver(spark: SparkSession, df: DataFrame) -> None:
    """Perform incremental MERGE into Silver Delta table."""

    if not DeltaTable.isDeltaTable(spark, SILVER_PATH):
        logger.info("Silver table not found → bootstrapping new table")
        (
            df.coalesce(1)
              .write.format("delta")
              .mode("overwrite")
              .partitionBy("year", "month", "day")
              .save(SILVER_PATH)
        )
        logger.info("Bootstrap completed.")
        return

    logger.info("Silver table exists → incremental MERGE")
    delta = DeltaTable.forPath(spark, SILVER_PATH)

    # Merge condition
    merge_cond = "target.order_number = source.order_number"

    # Only update if incoming row has higher priority
    update_cond = "source.status_priority > target.status_priority"

    cols = [
        "order_number", "adv_no", "trade_type", "asset", "fiat", "amount",
        "total_price", "unit_price", "order_status", "trade_time", "trade_date",
        "commission", "total_vnd", "net_volume", "year", "month", "day",
        "status_priority"
    ]

    set_map = {c: col(f"source.{c}") for c in cols}
    insert_map = {c: col(f"source.{c}") for c in cols}

    (
        delta.alias("target")
             .merge(df.alias("source"), merge_cond)
             .whenMatchedUpdate(condition=update_cond, set=set_map)
             .whenNotMatchedInsert(values=insert_map)
             .execute()
    )

    logger.info("MERGE completed.")


# ========================= MAIN ==========================================

def main():
    try:
        spark = create_spark_session()
        spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

        logger.info("Checking MinIO connectivity...")
        check_minio_connectivity(MINIO_ENDPOINT, ACCESS_KEY, SECRET_KEY, SILVER_PATH)

        # Read input parquet
        df = spark.read.parquet(INPUT_PATH)

        # Transform
        df_silver = transform_silver(df)

        # Merge
        merge_into_silver(spark, df_silver)

        logger.info("Silver job completed successfully.")

    except Exception as e:
        logger.exception("Silver job failed: %s", e)
        raise

    finally:
        if "spark" in locals():
            spark.stop()
        logger.info("Spark session stopped.")


if __name__ == "__main__":
    main()
