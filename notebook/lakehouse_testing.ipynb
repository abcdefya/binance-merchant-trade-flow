{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ddccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77761637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa18ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.minio_uploader import MinioUploader\n",
    "from src.utils.delta_exporter import *\n",
    "from src.components.data_ingestion import *\n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from binance_sdk_c2c.c2c import C2C, ConfigurationRestAPI, C2C_REST_API_PROD_URL\n",
    "from turtle import home\n",
    "from typing import List, Optional\n",
    "from binance_sdk_c2c.rest_api.models import GetC2CTradeHistoryResponse, GetC2CTradeHistoryResponseDataInner\n",
    "from src.utils.utils import *\n",
    "from src.components.data_ingestion import C2CExtended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f6400",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ab8864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Page 1 rate limits: []\n",
      "INFO:root:Page 1 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 2 rate limits: []\n",
      "INFO:root:Page 2 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 3 rate limits: []\n",
      "INFO:root:Page 3 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 4 rate limits: []\n",
      "INFO:root:Page 4 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 5 rate limits: []\n",
      "INFO:root:Page 5 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 6 rate limits: []\n",
      "INFO:root:Page 6 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 7 rate limits: []\n",
      "INFO:root:Page 7 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 8 rate limits: []\n",
      "INFO:root:Page 8 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 9 rate limits: []\n",
      "INFO:root:Page 9 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 10 rate limits: []\n",
      "INFO:root:Page 10 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 11 rate limits: []\n",
      "INFO:root:Page 11 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 12 rate limits: []\n",
      "INFO:root:Page 12 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 13 rate limits: []\n",
      "INFO:root:Page 13 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 14 rate limits: []\n",
      "INFO:root:Page 14 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 15 rate limits: []\n",
      "INFO:root:Page 15 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 16 rate limits: []\n",
      "INFO:root:Page 16 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 17 rate limits: []\n",
      "INFO:root:Page 17 retrieved 50 records, 50 within time range\n",
      "INFO:root:Page 18 rate limits: []\n",
      "INFO:root:Page 18 retrieved 50 records, 43 within time range\n",
      "INFO:root:Page 19 rate limits: []\n",
      "INFO:root:Page 19 retrieved 50 records, 0 within time range\n",
      "INFO:root:Consecutive empty pages: 1/3\n",
      "INFO:root:Page 20 rate limits: []\n",
      "INFO:root:Page 20 retrieved 50 records, 0 within time range\n",
      "INFO:root:Consecutive empty pages: 2/3\n",
      "INFO:root:Page 21 rate limits: []\n",
      "INFO:root:Page 21 retrieved 50 records, 0 within time range\n",
      "INFO:root:Consecutive empty pages: 3/3\n",
      "INFO:root:Stopping early: 3 consecutive pages with no data in time range\n"
     ]
    }
   ],
   "source": [
    "from binance_common.configuration import ConfigurationRestAPI\n",
    "# Create configuration for the REST API\n",
    "configuration_rest_api = ConfigurationRestAPI(\n",
    "    api_key=os.getenv(\"API_KEY\", \"\"),\n",
    "    api_secret=os.getenv(\"API_SECRET\", \"\"),\n",
    "    base_path=os.getenv(\"BASE_PATH\", C2C_REST_API_PROD_URL),\n",
    ")\n",
    "c2c = C2CExtended(configuration_rest_api)\n",
    "# Ví dụ 5: Lấy giao dịch khoảng thời gian tùy chỉnh (01/09/2025 - 10/09/2025)\n",
    "# Lấy giao dịch cho danh sách các ngày bất kỳ\n",
    "data = c2c.get_latest_by_week()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608a38a",
   "metadata": {},
   "source": [
    "## Convert API's response data to deltalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09b3c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e58b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 17:24:25.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.delta_exporter\u001b[0m:\u001b[36mapi_results_to_dataframe\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mCreated DataFrame: shape=(893, 18)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 1) Convert API results (list[dict] or objects) to DataFrame\n",
    "df = api_results_to_dataframe(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707a9371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8777addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = \"test_batch_transformation/delta/c2c_trades\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix cho Windows: Set HADOOP_HOME để tránh lỗi\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Tạo thư mục tạm cho Hadoop nếu đang dùng Windows\n",
    "if sys.platform.startswith('win'):\n",
    "    import tempfile\n",
    "    hadoop_home = os.path.join(tempfile.gettempdir(), 'hadoop')\n",
    "    os.makedirs(os.path.join(hadoop_home, 'bin'), exist_ok=True)\n",
    "    os.environ['HADOOP_HOME'] = hadoop_home\n",
    "    print(f\"Set HADOOP_HOME to: {hadoop_home}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bdefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo SparkSession với cấu hình cho Windows\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"C2C Trade Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/Working/binance-merchant-trading-flow/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(f\"✓ Spark running successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc file parquet từ Delta Lake\n",
    "delta_path = \"test_batch_transformation/delta/c2c_trades\"\n",
    "\n",
    "# Đọc Delta table\n",
    "df_spark = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print(f\"Number of records: {df_spark.count()}\")\n",
    "print(f\"Number of columns: {len(df_spark.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45398dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị schema của DataFrame\n",
    "df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f770980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị 10 bản ghi đầu tiên\n",
    "df_spark.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad266e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoặc nếu bạn chỉ muốn đọc trực tiếp file parquet (không qua Delta Lake)\n",
    "# df_spark = spark.read.parquet(\"test_batch_transformation/delta/c2c_trades/data.parquet\")\n",
    "# df_spark.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê cơ bản\n",
    "df_spark.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a05b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi sang Pandas DataFrame để xem dữ liệu dễ hơn (chỉ với dataset nhỏ)\n",
    "df_pandas = df_spark.toPandas()\n",
    "df_pandas.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55802c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-18 15:12:40.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.delta_exporter\u001b[0m:\u001b[36mwrite_dataframe_to_delta\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mWrote DataFrame to Delta: path=test_batch_transformation/delta/c2c_trades, mode=append, rows=893\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_batch_transformation/delta/c2c_trades'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_dataframe_to_delta(df, delta_path, mode=\"append\", partition_by=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13caee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bucket = \"spark_bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b644035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader = MinioUploader(endpoint = \"localhost:9000\", access_key = \"adminc2c\", secret_key = \"adminc2c\", secure = False, region = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5563f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 16:55:42.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.minio_uploader\u001b[0m:\u001b[36mensure_bucket\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mCreated bucket: test-bucket\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-17 16:55:42.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.minio_uploader\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mUploaded file d:\\Working\\binance-merchant-trading-flow\\test_data_lakehouse\\delta\\c2c_trades\\part-00001-56781e34-cd46-48fe-ad42-0b397e9c14ca-c000.snappy.parquet -> s3://test-bucket/test_data_lakehouse/delta/c2c_trades/part-00001-56781e34-cd46-48fe-ad42-0b397e9c14ca-c000.snappy.parquet\u001b[0m\n",
      "\u001b[32m2025-10-17 16:55:42.220\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.minio_uploader\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mUploaded file d:\\Working\\binance-merchant-trading-flow\\test_data_lakehouse\\delta\\c2c_trades\\part-00001-fef68d1f-e4de-4ccd-b223-3e84782218c0-c000.snappy.parquet -> s3://test-bucket/test_data_lakehouse/delta/c2c_trades/part-00001-fef68d1f-e4de-4ccd-b223-3e84782218c0-c000.snappy.parquet\u001b[0m\n",
      "\u001b[32m2025-10-17 16:55:42.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.minio_uploader\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mUploaded file d:\\Working\\binance-merchant-trading-flow\\test_data_lakehouse\\delta\\c2c_trades\\_delta_log\\00000000000000000000.json -> s3://test-bucket/test_data_lakehouse/delta/c2c_trades/_delta_log/00000000000000000000.json\u001b[0m\n",
      "\u001b[32m2025-10-17 16:55:42.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.minio_uploader\u001b[0m:\u001b[36mupload_file\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mUploaded file d:\\Working\\binance-merchant-trading-flow\\test_data_lakehouse\\delta\\c2c_trades\\_delta_log\\00000000000000000001.json -> s3://test-bucket/test_data_lakehouse/delta/c2c_trades/_delta_log/00000000000000000001.json\u001b[0m\n",
      "\u001b[32m2025-10-17 16:55:42.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.delta_exporter\u001b[0m:\u001b[36mupload_delta_to_minio\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mUploaded Delta folder to s3://test-bucket/test_data_lakehouse/delta/c2c_trades\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "upload_delta_to_minio(delta_path = delta_path, uploader = uploader, bucket = test_bucket, dest_prefix = \"test_data_lakehouse/delta/c2c_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17bb15",
   "metadata": {},
   "source": [
    "# Spark Transformation - Đọc và xử lý Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72547591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix cho Windows: Set HADOOP_HOME để tránh lỗi\n",
    "# Tạo thư mục tạm cho Hadoop nếu đang dùng Windows\n",
    "if sys.platform.startswith('win'):\n",
    "    import tempfile\n",
    "    hadoop_home = os.path.join(tempfile.gettempdir(), 'hadoop')\n",
    "    os.makedirs(os.path.join(hadoop_home, 'bin'), exist_ok=True)\n",
    "    os.environ['HADOOP_HOME'] = hadoop_home\n",
    "    print(f\"✓ Set HADOOP_HOME to: {hadoop_home}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e6661",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Tạo SparkSession với cấu hình Delta Lake\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC2C Trade Analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta.sql.DeltaSparkSessionExtension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.delta:delta-core_2.12:2.4.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark running successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Working\\binance-merchant-trading-flow\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1790)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:528)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:528)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:528)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "# Tạo SparkSession với cấu hình cho Windows\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"C2C Trade Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///D:/Working/binance-merchant-trading-flow/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(f\"✓ Spark running successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc file parquet từ Delta Lake\n",
    "delta_path = \"test_batch_transformation/delta/c2c_trades\"\n",
    "\n",
    "# Đọc Delta table\n",
    "df_spark = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print(f\"✓ Number of records: {df_spark.count()}\")\n",
    "print(f\"✓ Number of columns: {len(df_spark.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ec3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị schema của DataFrame\n",
    "df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiển thị 10 bản ghi đầu tiên\n",
    "df_spark.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các cách đọc parquet khác:\n",
    "\n",
    "# Cách 1: Đọc trực tiếp file parquet (không qua Delta Lake)\n",
    "# df_parquet = spark.read.parquet(\"test_batch_transformation/delta/c2c_trades/data.parquet\")\n",
    "# df_parquet.show(5)\n",
    "\n",
    "# Cách 2: Đọc tất cả file parquet trong thư mục\n",
    "# df_all = spark.read.parquet(\"test_batch_transformation/delta/c2c_trades/*.parquet\")\n",
    "# df_all.show(5)\n",
    "\n",
    "# Cách 3: Đọc với schema cụ thể\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "# custom_schema = StructType([...])\n",
    "# df_custom = spark.read.schema(custom_schema).parquet(\"path/to/file.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi sang Pandas DataFrame để xem dữ liệu dễ hơn (chỉ với dataset nhỏ)\n",
    "df_pandas = df_spark.limit(10).toPandas()\n",
    "df_pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5043a",
   "metadata": {},
   "source": [
    "## Một số phép biến đổi với Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thống kê cơ bản\n",
    "df_spark.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d60487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn một số cột cụ thể\n",
    "df_select = df_spark.select(\"orderNumber\", \"fiatType\", \"totalPrice\", \"price\", \"createTime\")\n",
    "df_select.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd74982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lọc dữ liệu\n",
    "# Ví dụ: Lọc các giao dịch có fiatType là 'VND'\n",
    "df_filtered = df_spark.filter(col(\"fiatType\") == \"VND\")\n",
    "print(f\"Total VND transactions: {df_filtered.count()}\")\n",
    "df_filtered.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by và aggregate\n",
    "# Tính tổng giao dịch theo fiatType\n",
    "df_grouped = df_spark.groupBy(\"fiatType\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_transactions\"),\n",
    "        sum(\"totalPrice\").alias(\"total_amount\"),\n",
    "        avg(\"totalPrice\").alias(\"avg_amount\")\n",
    "    )\n",
    "df_grouped.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
